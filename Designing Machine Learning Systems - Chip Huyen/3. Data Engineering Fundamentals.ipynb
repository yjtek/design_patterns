{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 3: Data Engineering Fundamentals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- User input: Stuff that users upload (text, images, videos, egc)\n",
    "\n",
    "- System generated: logs, predictions etc\n",
    "    - How to process logs\n",
    "    - How to store logs\n",
    "\n",
    "- Internal databases\n",
    "    - How to query the databases on the fly? e.g. On Amazon, does query about \"frozen\" relate to the disney show or frozen food?\n",
    "\n",
    "- Data\n",
    "    - First party data: Data re: your users/customers\n",
    "    - Second party data: data collected by another company made available to you\n",
    "    - Third party data: public users who are not direct customers (e.g. Android's AAID, China's state supported CAID)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data formats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Format | Binary/Text | Human-readable | Example use case |\n",
    "| - | - | - | - |\n",
    "| JSON | Text | Yes | Everywhere |\n",
    "| CSV | Text | Yes | Everywhere |\n",
    "| Parquet | Binary | No | Hadoop, Amazon Redshift |\n",
    "| Avro | Binary primary | No | Hadoop |\n",
    "| Protobuf | Binary primary | No | Google/Tensorflow |\n",
    "| Pickle | Binary | No | Python/Pytorch Serialization |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Row vs Column Major?\n",
    "    - Just refers to whether consecutive objects are items in the same row, or column\n",
    "    - Depending on this, you can have faster row access, or column access\n",
    "    - For read heavy databases, you want faster column access (so you can only focus on data that you want)\n",
    "    - For write heavy databases, you want faster row access (because when you write, each item is probably belonging to the same data row)\n",
    "\n",
    "- Text vs Binary\n",
    "    - Text is more readable, but less space efficient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relational Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Usually stored as CSV or parquet\n",
    "- First normal form (1NF)\n",
    "    - Every value is a single-valued attribute\n",
    "    - Attribute domain doesn't change\n",
    "    - Unique name for every attribute\n",
    "    - Order does not matter\n",
    "- Second normal form (2NF)\n",
    "    - Reduce redundant data from getting stored (i.e. separate into separate table if there is duplicate data)\n",
    "    - Downside is that more joins are needed, but upside is that less space is used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NoSQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Relational DB is useful, but limited, because of the inflexibility of the schema\n",
    "- Often when things kep changing, NoSQL works better\n",
    "- This is most useful when your data comes in as self-contained documents, and relationships between documents are rare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Document Model\n",
    "    - Single continuous string (JSON, XML etc)\n",
    "    - All documents have a unique key used for retrieval\n",
    "    - All documents should have similar schema, but schema similarity is not enforced, giving you more flexibility\n",
    "    - Easier to use, but harder to do stuff like `joins` etc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Graph model\n",
    "    - Data stored as nodes, with relationships modelled as edges\n",
    "    - A picture is worth a thousand words so ![graphql](./artifacts/3_image.png) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Structured vs Unstructured"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Structured | Unstructured |\n",
    "| - | - |\n",
    "| Schema rigorously defined | No schema | \n",
    "| Easy to search/analyse | Fast Arrival | \n",
    "| Only handle data that follows schema | Handles data from any source | \n",
    "| Schema changes cause cascading failures | Offloads the problems with schema changes to downstream applications | \n",
    "| Stored in data warehouse | Stored in data lakes | "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Storage Engines and Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transactional vs Analytical Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- If your app provides a service (ride matching, food ordering etc), you may wish to process and upload data each time a transaction is made. This is known as Online Transaction Processing (OLTP)\n",
    "    - This usually requires low latency + high availability of tables\n",
    "    - Most people expect databases to be ACID (Atomicity, Consistency, Isolation, Durability)\n",
    "    - However, OLTP tables may wish to loosen ACID demands to meet the low latency + high availability requirements\n",
    "    - Non ACID tables are sometimes known as BASE (Basically Available, Soft State, Eventual Consistency)\n",
    "\n",
    "- OLTP tables, however, may not serve the need for analytics. For example, if I want to count the number of rides in the last hour, you need to aggregate data across multiple rows/columns\n",
    "    - For such queries, analytical databases are needed, known as Online Analytical Processing (OLAP) databases\n",
    "\n",
    "- In recent years, trend has shifted away from these\n",
    "    - Both OLTP and OLAP databases have been combined into databases that support both (CockroachDB, DuckDB etc)\n",
    "    - Removal of coupling between storage and processing\n",
    "    - Online processing (i.e. real time processing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ETL: Extract Transform, and Load"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ETL is kind of a dated term; in the past you extract the data, transform into some format, and load into a database/data warehouse\n",
    "\n",
    "- Then came ELT, where you load things as an unprocessed mess into data lake first, then transform it to suit your needs\n",
    "\n",
    "- These days, solutions are managed hybrids (e.g. Snowflake, Databricks) that provide both data lake and data warehousing services on demand"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How does data flow between processes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Through Databases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Only feasible if both processes can access the same DB\n",
    "- DB Reads/Writes is typically slow, so latency sensitive applications won't be able to do this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Through REST/RPC APIs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- If you communicate through requests, you need the data to be small and secure enough to be sent via network\n",
    "- You also need to couple the 2 services tightly. If anything changes in one service in the communication phase, both are impacted\n",
    "- Nonetheless, microservices are considered best practise today simply because of the ease of setting them up once the contract is finalised"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Through real-time transports (Apache Kafka / Amazon Kinesis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- API based microservices fail when the request volume gets super large\n",
    "    - This is because the request model is fundamentally synchronous; the requestor and the receiver both lock until the request is fulfilled\n",
    "    - When services go down, all data is lost, and all downstream services go down together. Tightly coupled systems!!\n",
    "\n",
    "- To manage this, newer systems rely on a broker to pass messages to each other\n",
    "    - Data is broadcasted to the broker when something happens (called an **event**)\n",
    "    - When an event occurs, data is produced. Systems do not care who is consuming the data, only that it is produced\n",
    "    - This is a pub/sub system\n",
    "    - Data will be retained by broker for some pre-specified number of days before being deleted, or moved to permanent storage\n",
    "    - Example of pubsub solutions are Kafka, Amazon Kinesis, RabbitMQ etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch vs Stream Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- You can either process your data in batch (airflow cron jobs)\n",
    "    - Lower frequency, but more reliables\n",
    "    - Suitable for non-time sensitive data\n",
    "- Or process as a stream (FlinkSQL)\n",
    "    - Time sensitive data\n",
    "    - But expensive!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
