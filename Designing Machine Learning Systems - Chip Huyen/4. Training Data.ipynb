{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 4: Training Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- To train anything, you need data. But training on everything is expensive and slow\n",
    "- So you need to sample your data for better iteration!\n",
    "- There are many ways to sample your data, but keep in mind that the entire point of sampling is that your data should be representative of the population of interest. Make sure you don't end up with sample bias!!\n",
    "\n",
    "- Types of sampling\n",
    "    - Non-probability sampling: These are the many ways of saying that I sample anyhowly\n",
    "        - Convenience sample: Whatever is available\n",
    "        - Snowball sample: Future samples based on current samples\n",
    "        - Judgement sample: Someone decides what to sample\n",
    "        - Quota sample: Sample based on quota, but no randomisation\n",
    "\n",
    "    - Simple random sample\n",
    "    - Stratified sample: based on some baseline population\n",
    "    - Weighted sample: reweighting your population in some way\n",
    "    - Reservoir sample: This is an interesting one, especially for streaming data\n",
    "        - Problem: You have an incoming stream of tweets, and you want to sample some $K$ of them. You don't know how many tweets there are, but you cannot fit them all into memory. How can you make sure that every tweet has equal probability of selection, no matter when you stop the sampling?\n",
    "        - Solution:\n",
    "            - Make an array of size $K$. Put the first $K$ elements into the array\n",
    "            - For the next element $K+1$, generate a random number $i$ such that $1 \\le i \\le K+1$\n",
    "            - If $1 \\le i \\le K$, swap the i-th element with the new element\n",
    "            - At each step $N$, every element has $\\frac{K}{N}$ chance of getting put in/out of the array \n",
    "\n",
    "    - Importance sample: See subsection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importance Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Importance sampling is a very very important concept to learn, so it gets its own subsection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Labeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Labelling is a core function of ML, because supervised learning is entirely dependent on label quality\n",
    "- How to label?\n",
    "    - Hand labelling: Expensive, not private, and slow\n",
    "    - Label multiplicity: Often, to augment the usual hand-labelling method, you gather labels from multiple sources, leading to multiple different levels of expertise and accuracy, and, worse still, labels may contradict each other\n",
    "        - Data Lineage: Without a good sense of your data lineage, you don't know when your data is reliable. Which can cause your data to fail without you understanding why\n",
    "    - Natural labels: As far as possible, try to get labels from \"nature\", or at least from actions that people perform spontaneously\n",
    "    - Window length: Your labels may not be instantaneous. Poeple read books/watch videos at different speeds. So the presence of a label may be a function of quality, but also a function of how long they take to finish consuming it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- How to handle missing labels?\n",
    "\n",
    "| Method | How | Ground truths required? | \n",
    "| - | - | - |\n",
    "| Weak supervision | Use heuristics to generate labels | No, but small number of labels are recommended to guide the development of heuristics | \n",
    "| Semi-supervision | Structural assumptions to generate labels | Yes, small number of initial labels as seeds. Use the small number of labelled data to train a model, predict on unlabelled data, and add new labelled data (with high predicted confidence) to the training |\n",
    "| Transfer learning | Use pre-trained models for new task | No if zero-shot learning. Yes for fine-tuning, but less than supervised case |\n",
    "| Active learning | Label data most useful to your model | Yes. The idea here is to allow your machine learning model to send back queries to the human labeller to ask for more information on samples it is unsure about. |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class Imbalance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The usual imbalanced class problems apply\n",
    "    - Need for better evaluation (i.e. predicting all majority class gives you 99.99% accuracy)\n",
    "    - Insufficient data for minority classification\n",
    "    - Asymmetric cost of error; cost of mis-prediction of rare class may be much higher than wrong prediction of majority class\n",
    "\n",
    "- Solutions to imbalance\n",
    "    - Brute force: Bigger NNs seem to build better world representations, so deal with imbalance better\n",
    "    - Using better evaluation metrics: F1 + prec/recall instead of accuracy\n",
    "    - Resampling data: Please don't. The results are terrible\n",
    "    - Modify loss function: The usual loss function used is $L(X; \\theta) = \\sum_x \\frac{1}{N} L(x; \\theta)$\n",
    "        - Cost sensitive learning: $L(X; \\theta) = \\sum_j C_{ij} P(j | x; \\theta)$\n",
    "            - Compute the cost of classifying an observation with label $i$ as label $j$...\n",
    "            - ...weighted by the probability of labelling it $j$\n",
    "        - Class-weighted loss: Same idea as cost sensitive learning, but instead weight by the prevalence of the class\n",
    "        - Focal loss: Increase weight for observations that have a lower probability of being right"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Label preserving augmentation\n",
    "    - Rotate/crop/modify some image, while preserving labels\n",
    "\n",
    "- Perturbation\n",
    "    - Randomly add noise to the data while retaining the same label\n",
    "\n",
    "- Data synthesis\n",
    "    - Use conversational AI to bootstrap training data in NLP"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
