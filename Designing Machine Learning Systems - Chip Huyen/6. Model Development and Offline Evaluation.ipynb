{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 6: Model Development and Offline Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating ML Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Not just in terms of metrics (accuracy, F1 etc), but also in terms of stuff like data, compute, training time, inference latency, and interpretability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tips to select models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Don't worry about SOTA. Worry about what works\n",
    "- Start with simple models\n",
    "- Avoid human biases e.g. you spend more time developing models that are \"newer\", so it performs better\n",
    "    - When comparing different architectures, make sure it's compared under comparable setups\n",
    "- Good performance now vs good performance later\n",
    "    - Plot learning curve (i.e. score against sample count), to show you the marginal contribution of more training data\n",
    "\n",
    "- What are your tradeoffs?\n",
    "    - False positives vs false negative trade off\n",
    "    - Compute requirement vs accuracy tradeoff\n",
    "\n",
    "- What are your model's assumptions?\n",
    "    - IID?\n",
    "    - Smoothness?\n",
    "    - Boundary shapes?\n",
    "    - Conditional independence?\n",
    "    - Normal distirbution?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensembles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Ensembling usually performs great on desktop, but not in production, because it is quite hard to maintain\n",
    "- But generally, whenever you want to have some kind of marginal gain, ensembling should be your go to first step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Bookstrap aggregation of raw observations will heolp you reduce variance in model predictions, and avoid overfitting\n",
    "- Rather than training on only 1 dataset, create multiple datasets through resampling the data you have, then average over all the models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- XGBoost/LightGBM handles this out of the box\n",
    "- Basically the idea is to train many lousy (simple) classifiers and recombine them to make a strong classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stacking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Train base learnings from training data, then create a metalearner to combine the output of the base learners\n",
    "- Kind of light ensembling, except instead of equal weights you train some complex set of combination"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment Tracking/Versioning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Best to use something like MLFlow to track and version your model training, so you can keep track of features used/parameters tried. Log things like\n",
    "    - Loss curve\n",
    "    - Model performance\n",
    "    - Samples used\n",
    "    - prediction vs ground truth\n",
    "    - Speed (training and inference)\n",
    "    - System metrics (CPU utilisation)\n",
    "    - Hyperparameters\n",
    "\n",
    "- Versioning\n",
    "    - Versioning ML Models and data used is impt, because data will shift, and you will have multiple copies of the same model on different days (DVC)\n",
    "\n",
    "- ML fails in many ways\n",
    "    - Poor data\n",
    "    - Poor implementation\n",
    "    - Pipeline failures\n",
    "    - Hyperparameter failures\n",
    "    - Feature choice failures \n",
    "    - etc\n",
    "\n",
    "- To debug\n",
    "    - Start simple and add more components\n",
    "    - Purposely overfit a single batch, to make sure that you implementation is at least correct\n",
    "    - Use random seeds to test performance over a variety of runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distributed training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- You can train models across multiple machines, for faster training speed\n",
    "\n",
    "- Split data on multiple machines, train local models, then accumulate gradients\n",
    "\n",
    "![async train](./artifacts/6_image.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Split model on different machines and train if tasks are parallel (e.g. each layer of an FFN can be trained on a different model)\n",
    "\n",
    "- Split each part of a pipeline to be trained on different machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AutoML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- AutoML is a catchall buzzword\n",
    "\n",
    "- But what's interesting is how automl can be used to automate parts of the model building\n",
    "    - Automate feature selection?\n",
    "    - Automate architecture search? \n",
    "    - etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Offline Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- How do you know that your models are actually working? \n",
    "\n",
    "- Often, data in production does not give you ground truth\n",
    "    - e.g. if you already chose NOT to lend money to someone,it is not possible to have a ground truth label of credit worthiness\n",
    "\n",
    "- Solutions:\n",
    "    - Baselines: \n",
    "        - Let's say your model is 90% accurate. What if you had random labelling? What is the uplift?\n",
    "        - What about some simple heuristic?\n",
    "        - How about some human baseline?\n",
    "        - Simple model?\n",
    "\n",
    "- Evaluation - do the labels flip dramatically when:\n",
    "    - What happens to predictions if I randomly perturb my data?\n",
    "    - What happens when I remove some features from the model? \n",
    "    - Does \"70%\" probability really mean 70% in your model output? Calibrate it properly\n",
    "    - Confidence level: you may only want to take action on model outputs that you are confident about\n",
    "    - Subset evaluation: Are there specific groups that your model underperforms on?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
