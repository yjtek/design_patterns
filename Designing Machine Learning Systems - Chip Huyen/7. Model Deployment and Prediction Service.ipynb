{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 7: Model Deployment and Prediction Service"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Deployment can be as easy as using Flask/FastAPI to write some boilerplate\n",
    "    ```python\n",
    "    @app.route('/predict', methods=['POST'])\n",
    "    def predict():\n",
    "        X = request.get_json()['X']\n",
    "        y = MODEL.predict(X).tolist()\n",
    "        return json.dumps({'y': y}), 200\n",
    "    ```\n",
    "\n",
    "- But what if you need your model to be highly available, with 99% uptime, and setting up infra so people are notified when things go wrong?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deployment Myths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Myth: You only deploy 1 or 2 models in prd \n",
    "    - tbh even a simple service like GFG has hundreds of models\n",
    "- Myth: Model work ends upon deployment\n",
    "    - Your model probably sucks, and will suck more over time\n",
    "- Myth: You won't need to update your model\n",
    "    - See the previous myth\n",
    "- Myth: Most MLEs don't need to worry about scale\n",
    "    - Yea that's completely untrue. Nobody is going to accept that doubling a product reach is going to double costs, when the entire point of ML is scalability to begin with"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch vs Online"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Online prediction: On demand\n",
    "- Batch prediction: Batch requests to infer at fixed intervals\n",
    "- Often done side by side"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unified Batch and Streaming Pipeline "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![batch v stream](./artifacts/7_image.png)\n",
    "\n",
    "![system](./artifacts/7a_image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Compression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Low rank factorisation\n",
    "    - When you have blocks that are super large (e.g. big layers in neural nets), it can make sense to size down your components\n",
    "    - e.g. Instead of 3x3 convolutions, do 1x1 convolutions\n",
    "\n",
    "- Knowledge distillation\n",
    "    - Train a model (student) to mimic the prediction of a bigger model (teacher)\n",
    "\n",
    "- Pruning\n",
    "    - Remove unnecessary parts of the model (e.g. layers with small to zero weights)\n",
    "\n",
    "- Quantization\n",
    "    - Change 64 bit float to 8 bit float\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML on Edge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Instead of running inference on your own compute, offload to edge devices\n",
    "\n",
    "- This requires specialised skill set to map compute to hardware"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![memory](./artifacts/7b_image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- To avoid a specific framework for each type of compute primitive, best to rely on a middleman framework, which translates high to low level intermediate representations before translating to machine code\n",
    "\n",
    "- This is known as \"lowering\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Optimisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- There is sometimes an additional step between MLEs and Data Scientist, and these are optimisation engineers\n",
    "- Basically taking single-threaded models and rewriting components to make them run faster\n",
    "\n",
    "- Some standard optimisation techniques:\n",
    "    - Vectorisation: Given a loop or a nested loop, instead of executing it one item at a time, execute multiple elements contiguous in memory at the same time to reduce latency caused by data I/O.\n",
    "    - Parallelization: Operate on multiple chunks of the data at the same time\n",
    "    - Loop tiling: Change the data accessing order in a loop to leverage hardwareâ€™s memory layout and cache. This kind of optimization is hardware dependent: A good access pattern on CPUs is not a good access pattern on GPUs.\n",
    "    - Operator fusion: Fuse multiple operators into one to avoid redundant memory access."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ML for Optimisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For example, in CUDA, there is `torch.backends.cudnn.benchmark=True` which enables cuDNN autotune\n",
    "- `autoTVM` is another such solution that tunes the architecture of your model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ML in Browsers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- WebAssembly is the often used language to run ML in browsers, though this is still slow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![big picture](./artifacts/7d_image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
